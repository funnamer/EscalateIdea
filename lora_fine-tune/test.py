import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "../model/lora_output"  
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ğŸš€ Loading model from:", model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.to(device)
model.eval()


system_prompt = (
    "ä½ æ˜¯ä¸€ä½ä¸´åºŠåŒ»å­¦ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®æ‚£è€…æè¿°æä¾›åˆæ­¥è¯Šæ–­å’Œå¥åº·å»ºè®®ã€‚"
    "è¯·ç”¨ç®€æ´ã€é€»è¾‘æ¸…æ™°çš„ä¸­æ–‡å›ç­”ï¼Œä¸è¶…è¿‡150å­—ã€‚â€ã€‚"
)
user_input = "5æœˆè‡³ä»Šä¸Šè…¹é å³éšç—›ï¼Œå³èƒŒéšç—›å¸¦é…¸ï¼Œä¾¿ç§˜ï¼Œå–œç¡ï¼Œæ—¶æœ‰è…¹ç—›ï¼Œå¤´ç—›ï¼Œè…°é…¸ç—‡çŠ¶ï¼Ÿ"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_input}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,enable_thinking=False)

# ç”Ÿæˆè¾“å…¥å¼ é‡
model_inputs = tokenizer([text], return_tensors="pt", padding=True).to(device)

generation_config = dict(
    max_new_tokens=512,        
    temperature=0.4,         
    top_p=0.8,              
    top_k=8,              
    repetition_penalty=1.1,    
    do_sample=True,            
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    attention_mask=model_inputs["attention_mask"],  
)
with torch.no_grad():
    output_ids = model.generate(
        input_ids=model_inputs.input_ids,
        **generation_config
    )

generated_ids = [
    output[len(input):] for input, output in zip(model_inputs.input_ids, output_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()


if "è°¢è°¢" in response:
    response = response.split("è°¢è°¢")[0] + "è°¢è°¢ä½ çš„æé—®ã€‚"
elif len(response) > 200:
    response = response[:200] + "â€¦â€¦è°¢è°¢ä½ çš„æé—®ã€‚"

print("\n===============================")
print("âœ… æ¨¡å‹è¾“å‡ºï¼š\n")
print(response)
print("===============================")

