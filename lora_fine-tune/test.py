# -*- coding: utf-8 -*-
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ========== åŸºç¡€é…ç½® ==========
model_path = "../model/lora_output"   # å·²èåˆåçš„æ¨¡å‹è·¯å¾„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ğŸš€ Loading model from:", model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_path)
model.to(device)
model.eval()

# ========== æ„é€  Prompt ==========
system_prompt = (
    "ä½ æ˜¯ä¸€ä½ä¸´åºŠåŒ»å­¦ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®æ‚£è€…æè¿°æä¾›åˆæ­¥è¯Šæ–­å’Œå¥åº·å»ºè®®ã€‚"
    "è¯·ç”¨ç®€æ´ã€é€»è¾‘æ¸…æ™°çš„ä¸­æ–‡å›ç­”ï¼Œä¸è¶…è¿‡150å­—ã€‚â€ã€‚"
)
user_input = "5æœˆè‡³ä»Šä¸Šè…¹é å³éšç—›ï¼Œå³èƒŒéšç—›å¸¦é…¸ï¼Œä¾¿ç§˜ï¼Œå–œç¡ï¼Œæ—¶æœ‰è…¹ç—›ï¼Œå¤´ç—›ï¼Œè…°é…¸ç—‡çŠ¶ï¼Ÿ"

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_input}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,enable_thinking=False)

# ç”Ÿæˆè¾“å…¥å¼ é‡
model_inputs = tokenizer([text], return_tensors="pt", padding=True).to(device)

# ========== ç”Ÿæˆå‚æ•° ==========
generation_config = dict(
    max_new_tokens=512,         # è¶³å¤Ÿç©ºé—´è®©æ¨¡å‹æ”¶å°¾
    temperature=0.4,           # æ§åˆ¶éšæœºæ€§ï¼ˆè¶Šä½è¶Šç¨³ï¼‰
    top_p=0.8,                 # nucleusé‡‡æ ·
    top_k=8,                   # é™åˆ¶é‡‡æ ·èŒƒå›´
    repetition_penalty=1.1,     # é˜²æ­¢é‡å¤ä¸å•°å—¦
    do_sample=True,             # å¯ç”¨é‡‡æ ·
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.eos_token_id,
    attention_mask=model_inputs["attention_mask"],  # é˜²æ­¢maskè­¦å‘Šä¸æˆªæ–­
)

# æ¨ç†éƒ¨åˆ†
with torch.no_grad():
    output_ids = model.generate(
        input_ids=model_inputs.input_ids,
        **generation_config
    )

#å»æ‰è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
generated_ids = [
    output[len(input):] for input, output in zip(model_inputs.input_ids, output_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()


# æˆªæ–­å¤šä½™å†…å®¹ï¼Œè®©è¾“å‡ºæ›´é›†ä¸­
if "è°¢è°¢" in response:
    response = response.split("è°¢è°¢")[0] + "è°¢è°¢ä½ çš„æé—®ã€‚"
elif len(response) > 200:
    response = response[:200] + "â€¦â€¦è°¢è°¢ä½ çš„æé—®ã€‚"

print("\n===============================")
print("âœ… æ¨¡å‹è¾“å‡ºï¼š\n")
print(response)
print("===============================")
